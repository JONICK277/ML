{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JONICK277/ML/blob/main/model_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e8aa1d",
      "metadata": {
        "id": "a5e8aa1d"
      },
      "source": [
        "# Model Evaluation with Nested Cross-Validation\n",
        "\n",
        "This notebook performs model evaluation using nested cross-validation for hyperparameter tuning.\n",
        "It then evaluates each model on a held-out validation set and generates predictions on an external test set,\n",
        "saving the results as Excel files for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7f16833",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7f16833",
        "outputId": "eb9854b4-5122-4e4e-ef43-7f87b8ceb151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 104 (delta 13), reused 29 (delta 6), pack-reused 50 (from 1)\u001b[K\n",
            "Receiving objects: 100% (104/104), 113.56 MiB | 11.35 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Updating files: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from math import sqrt\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- Data Download ---\n",
        "# Execute this if you are running the notebook in Google Colab\n",
        "!git clone https://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@github.com/JONICK277/ML.git\n",
        "\n",
        "# Load the cleaned data\n",
        "train_cleaned = pd.read_pickle(\"ML/data/cleaned/train/train_cleaned.pkl\")\n",
        "test_cleaned  = pd.read_pickle(\"ML/data/cleaned/test/test_cleaned.pkl\")\n",
        "# Alternatively, if running locally, uncomment the following:\n",
        "# train_cleaned = pd.read_pickle(\"../../data/cleaned/train/train_cleaned.pkl\")\n",
        "# test_cleaned  = pd.read_pickle(\"../../data/cleaned/test/test_cleaned.pkl\")\n",
        "\n",
        "# --- Preparation ---\n",
        "target = \"LAID_UP_TIME\"\n",
        "X = train_cleaned.drop(columns=[target])\n",
        "y = train_cleaned[target]\n",
        "\n",
        "# Split the cleaned training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8459aa95",
      "metadata": {
        "id": "8459aa95"
      },
      "source": [
        "## Nested Cross-Validation Function\n",
        "\n",
        "This function performs nested cross-validation: the outer loop estimates the generalization error,\n",
        "and the inner loop tunes hyperparameters using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b75d6416",
      "metadata": {
        "id": "b75d6416"
      },
      "outputs": [],
      "source": [
        "def nested_cv(model, param_grid, X, y, cv_outer=5, cv_inner=3):\n",
        "    \"\"\"\n",
        "    Perform nested cross-validation.\n",
        "    Returns a list of RMSE scores for each outer fold.\n",
        "    \"\"\"\n",
        "    outer_cv = KFold(n_splits=cv_outer, shuffle=True, random_state=42)\n",
        "    outer_rmse = []\n",
        "\n",
        "    for train_idx, test_idx in outer_cv.split(X):\n",
        "        X_outer_train, X_outer_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_outer_train, y_outer_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Inner CV for hyperparameter tuning using GridSearchCV\n",
        "        inner_cv = KFold(n_splits=cv_inner, shuffle=True, random_state=42)\n",
        "        gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv,\n",
        "                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "        gs.fit(X_outer_train, y_outer_train)\n",
        "\n",
        "        best_model = gs.best_estimator_\n",
        "        y_pred_outer = best_model.predict(X_outer_test)\n",
        "        rmse = sqrt(mean_squared_error(y_outer_test, y_pred_outer))\n",
        "        outer_rmse.append(rmse)\n",
        "        print(f\"Outer fold RMSE: {rmse:.4f}\")\n",
        "\n",
        "    return outer_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca3d847",
      "metadata": {
        "id": "dca3d847"
      },
      "source": [
        "## Model 1: Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "872e6a45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "872e6a45",
        "outputId": "c8b0f42f-b88e-42a8-9d2c-a6af91404177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bootstrap': [True, False],\n",
            " 'max_depth': [10, 20, 30],\n",
            " 'max_features': ['sqrt'],\n",
            " 'min_samples_leaf': [2, 5, 10],\n",
            " 'min_samples_split': [5, 10, 20],\n",
            " 'n_estimators': [500, 700, 800, 1000]}\n",
            "\n",
            "--- Random Forest Nested CV ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-52cfc2a8ae16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Random Forest Nested CV ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mrf_rmse_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_outer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random Forest Nested CV RMSE scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_rmse_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean RF Nested RMSE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_rmse_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-24f1aa74749e>\u001b[0m in \u001b[0;36mnested_cv\u001b[0;34m(model, param_grid, X, y, cv_outer, cv_inner)\u001b[0m\n\u001b[1;32m     15\u001b[0m         gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv,\n\u001b[1;32m     16\u001b[0m                           scoring='neg_mean_squared_error', n_jobs=-1)\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_outer_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_outer_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [500, 700, 800, 1000],\n",
        "    'max_features': ['sqrt'],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [5, 10, 20],\n",
        "    'min_samples_leaf': [2, 5, 10],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "pprint(param_grid_rf)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "print(\"\\n--- Random Forest Nested CV ---\")\n",
        "rf_rmse_scores = nested_cv(rf_model, param_grid_rf, X_train, y_train, cv_outer=5, cv_inner=3)\n",
        "print(\"Random Forest Nested CV RMSE scores:\", rf_rmse_scores)\n",
        "print(\"Mean RF Nested RMSE:\", np.mean(rf_rmse_scores))\n",
        "best_rf = rf_model.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Random Forest Final Evaluation\n",
        "\n",
        "y_val_pred_rf = best_rf.predict(X_val)\n",
        "val_rmse_rf = sqrt(mean_squared_error(y_val, y_val_pred_rf))\n",
        "print(\"\\nFinal Validation RMSE (Random Forest):\", val_rmse_rf)"
      ],
      "metadata": {
        "id": "O5WQO025TEd_"
      },
      "id": "O5WQO025TEd_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tuning on whole dataset for prediction"
      ],
      "metadata": {
        "id": "7AcQs7U0R4No"
      },
      "id": "7AcQs7U0R4No"
    },
    {
      "cell_type": "code",
      "source": [
        "# Final tuning on the entire training set and saving the best model for Random Forest\n",
        "rf_random = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "rf_random.fit(X, y)\n",
        "best_model_rf = rf_random.best_estimator_\n",
        "with open('best_model_forest.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model_rf, f)"
      ],
      "metadata": {
        "id": "O-_T85z0SDkp"
      },
      "id": "O-_T85z0SDkp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c4a754ee",
      "metadata": {
        "id": "c4a754ee"
      },
      "source": [
        "## Model 2: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3228f46c",
      "metadata": {
        "id": "3228f46c"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "    'max_depth': [3, 5, 7, 8, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.4],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "pprint(param_grid_xgb)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = XGBRegressor(tree_method='hist', device=\"cuda\", random_state=42)\n",
        "print(\"\\n--- XGBoost Nested CV ---\")\n",
        "xgb_rmse_scores = nested_cv(xgb_model, param_grid_xgb, X_train, y_train, cv_outer=5, cv_inner=3)\n",
        "print(\"XGBoost Nested CV RMSE scores:\", xgb_rmse_scores)\n",
        "print(\"Mean XGBoost Nested RMSE:\", np.mean(xgb_rmse_scores))\n",
        "best_xgb = xgb_random.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### XGBoost Final Evaluation\n",
        "y_val_pred_xgb = best_xgb.predict(X_val)\n",
        "val_rmse_xgb = sqrt(mean_squared_error(y_val, y_val_pred_xgb))\n",
        "print(\"\\nFinal Validation RMSE (XGBoost):\", val_rmse_xgb)"
      ],
      "metadata": {
        "id": "Nh0S1VzMTHbH"
      },
      "id": "Nh0S1VzMTHbH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tuning on whole dataset for prediction"
      ],
      "metadata": {
        "id": "gOm_ScurSJuI"
      },
      "id": "gOm_ScurSJuI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Final tuning on the entire training set and saving the best model for XGBoost\n",
        "xgb_random = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=3,\n",
        "                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "xgb_random.fit(X, y)\n",
        "best_model_xgb = xgb_random.best_estimator_\n",
        "with open('best_model_xgboost.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model_xgb, f)"
      ],
      "metadata": {
        "id": "gEw2Pv7fSMEe"
      },
      "id": "gEw2Pv7fSMEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f6cfb1e3",
      "metadata": {
        "id": "f6cfb1e3"
      },
      "source": [
        "## Model 3: Gradient Boost (using XGBoost for GPU training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb2aa6b",
      "metadata": {
        "id": "fcb2aa6b"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameter grid for Gradient Boost\n",
        "param_grid_grad = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
        "    'max_depth': [3, 5, 7, 8, 9, 10],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.4],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'min_child_weight': [1, 3, 5, 6, 7, 8]\n",
        "}\n",
        "pprint(param_grid_grad)\n",
        "\n",
        "# Initialize the Gradient Boost model\n",
        "grad_model = XGBRegressor(tree_method='hist', device=\"cuda\", random_state=42)\n",
        "print(\"\\n--- Gradient Boost Nested CV ---\")\n",
        "grad_rmse_scores = nested_cv(grad_model, param_grid_grad, X_train, y_train, cv_outer=5, cv_inner=3)\n",
        "print(\"Gradient Boost Nested CV RMSE scores:\", grad_rmse_scores)\n",
        "print(\"Mean Gradient Boost Nested RMSE:\", np.mean(grad_rmse_scores))\n",
        "best_grad = grad_model.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Gradient Boost Final Evaluation\n",
        "y_val_pred_grad = best_grad.predict(X_val)\n",
        "val_rmse_grad = sqrt(mean_squared_error(y_val, y_val_pred_grad))\n",
        "print(\"\\nFinal Validation RMSE (Gradient Boost):\", val_rmse_grad)"
      ],
      "metadata": {
        "id": "fdL--r31TChG"
      },
      "id": "fdL--r31TChG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tuning on whole dataset for prediction"
      ],
      "metadata": {
        "id": "MDKXvjuQSKWe"
      },
      "id": "MDKXvjuQSKWe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Final tuning on the entire training set and saving the best model for Gradient Boost\n",
        "grad_random = GridSearchCV(estimator=grad_model, param_grid=param_grid_grad, cv=3,\n",
        "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grad_random.fit(X, y)\n",
        "best_model_grad = grad_random.best_estimator_\n",
        "with open('best_model_grad_boost_small_test_set.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model_grad, f)"
      ],
      "metadata": {
        "id": "kgyUsmZkSOKu"
      },
      "id": "kgyUsmZkSOKu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6f57fa6f",
      "metadata": {
        "id": "6f57fa6f"
      },
      "source": [
        "## Final Evaluation on Validation Set\n",
        "\n",
        "For each model, we load the saved best model, predict on the validation set, compute the RMSE,\n",
        "and then predict on the external test set. The results are saved to Excel files."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on external test set for Random Forest and save results\n",
        "with open('best_model_forest.pkl', 'rb') as f:\n",
        "    best_model_rf = pickle.load(f)\n",
        "\n",
        "test_cleaned_copy = test_cleaned.copy()\n",
        "chassis_number_rf = test_cleaned_copy['CHASSIS_NUMBER']\n",
        "test_cleaned_copy = test_cleaned_copy.drop(columns=['CHASSIS_NUMBER', 'LAID_UP_TIME'])\n",
        "y_test_pred_rf = best_model_rf.predict(test_cleaned_copy)\n",
        "result_rf = pd.DataFrame({\n",
        "    'CHASSIS_NUMBER': chassis_number_rf,\n",
        "    'LAID_UP_TIME': y_test_pred_rf\n",
        "})\n",
        "try:\n",
        "    with open(\"../../results/teamB-model1_RF.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_rf.to_excel(\"../../results/teamB-model1_RF.xlsx\", index=False)\n",
        "try:\n",
        "    with open(\"ML/results/teamB-model1_RF.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_rf.to_excel(\"ML/results/teamB-model1_RF.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "4VFlgTeOS2Xk"
      },
      "id": "4VFlgTeOS2Xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on external test set for XGBoost and save results\n",
        "with open('best_model_xgboost.pkl', 'rb') as f:\n",
        "    best_model_xgb = pickle.load(f)\n",
        "\n",
        "test_cleaned_copy = test_cleaned.copy()\n",
        "chassis_number_xgb = test_cleaned_copy['CHASSIS_NUMBER']\n",
        "test_cleaned_copy = test_cleaned_copy.drop(columns=['CHASSIS_NUMBER', 'LAID_UP_TIME'])\n",
        "y_test_pred_xgb = best_model_xgb.predict(test_cleaned_copy)\n",
        "result_xgb = pd.DataFrame({\n",
        "    'CHASSIS_NUMBER': chassis_number_xgb,\n",
        "    'LAID_UP_TIME': y_test_pred_xgb\n",
        "})\n",
        "try:\n",
        "    with open(\"../../results/teamB-model2_XGB.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_xgb.to_excel(\"../../results/teamB-model2_XGB.xlsx\", index=False)\n",
        "try:\n",
        "    with open(\"ML/results/teamB-model2_XGB.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_xgb.to_excel(\"ML/results/teamB-model2_XGB.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "i7cUadeOTW6n"
      },
      "id": "i7cUadeOTW6n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e25e8f7",
      "metadata": {
        "id": "3e25e8f7"
      },
      "outputs": [],
      "source": [
        "# Predict on external test set for Gradient Boost and save results\n",
        "with open('best_model_grad_boost_small_test_set.pkl', 'rb') as f:\n",
        "    best_model_grad = pickle.load(f)\n",
        "\n",
        "test_cleaned_copy = test_cleaned.copy()\n",
        "chassis_number_grad = test_cleaned_copy['CHASSIS_NUMBER']\n",
        "test_cleaned_copy = test_cleaned_copy.drop(columns=['CHASSIS_NUMBER', 'LAID_UP_TIME'])\n",
        "y_test_pred_grad = best_model_grad.predict(test_cleaned_copy)\n",
        "result_grad = pd.DataFrame({\n",
        "    'CHASSIS_NUMBER': chassis_number_grad,\n",
        "    'LAID_UP_TIME': y_test_pred_grad\n",
        "})\n",
        "try:\n",
        "    with open(\"../../results/teamB-model3_Grad.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_grad.to_excel(\"../../results/teamB-model3_Grad.xlsx\", index=False)\n",
        "try:\n",
        "    with open(\"ML/results/teamB-model3_Grad.xlsx\") as f:\n",
        "        raise FileExistsError\n",
        "except FileNotFoundError:\n",
        "    result_grad.to_excel(\"ML/results/teamB-model3_Grad.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}